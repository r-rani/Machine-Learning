{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3224e7-eaf1-4466-817b-149fd7ca1beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler() \n",
    "#XX train = sc.fit \n",
    "#XX transform(XX valid = sc.transform(XX train) valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de556982-6d7f-4665-9f5a-ac9555e1df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables \n",
    "SEED = 1161\n",
    "N_TRAIN = 9\n",
    "N_VALID = 100\n",
    "N_TEST  = 100\n",
    "OUTDIR = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7994a7-0b76-439c-8b81-6b3085ee3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equations \n",
    "def f_opt(x):\n",
    "    return np.sin(2.0 * np.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f38674-c94f-40c6-9eb9-5a60e314efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the test, vaildation, and training set \n",
    "def generate_sets(seed, N_train, N_valid, N_test):\n",
    "    x_tr = np.linspace(0, 1, N_train)\n",
    "    x_va = np.linspace(0, 1, N_valid)\n",
    "    x_te =np.linspace(0, 1, N_test)\n",
    "\n",
    "    t_tr = f_opt(x_tr) + 0.2*np.random.randn(N_train)\n",
    "    t_va = f_opt(x_va) + 0.2*np.random.randn(N_valid)\n",
    "    t_te = f_opt(x_te) + 0.2*np.random.randn(N_test)\n",
    "    return x_tr, t_tr, x_va, t_va, x_te, t_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce6265db-b522-4890-b0ee-3b2be689a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_matrix(x, M):\n",
    "    return np.vstack([x**m for m in range(M + 1)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba9a5d0-d2c6-46cc-bbc0-8b86f6158b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_least_squares(x, t):\n",
    "    A=np.dot(X.T, X)\n",
    "    A1=np.linalg.inv(A)\n",
    "    b=np.dot(X.T, t_train)\n",
    "    w=np.dot(A1,b)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981aa58a-210c-4555-8b7f-2737a701946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8391edd3-d101-46a4-b1d8-9338c12efbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_vs_truth(x_grid, y_model, x_tr, t_tr, x_va, t_va, title, filename):\n",
    "    \"\"\"\n",
    "    Plot a single fitted curve y_model against f_opt, with train/validation scatter.\n",
    "    One figure per M (unregularized) or per λ (ridge).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(x_grid, f_opt(x_grid), label=\"f_opt(x) = sin(2πx)\")\n",
    "    plt.plot(x_grid, y_model,        label=\"Model prediction\")\n",
    "    plt.scatter(x_tr, t_tr, label=\"Train\")\n",
    "    plt.scatter(x_va, t_va, label=\"Validation\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"t / prediction\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTDIR}/{filename}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371a8d93-817b-49d7-8560-8551461420c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmse_vs_M(Ms, rmse_train, rmse_val, fopt_val_rmse, filename):\n",
    "    \"\"\"\n",
    "    Plot train/validation RMSE vs M with a horizontal line for f_opt's validation RMSE.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(Ms, rmse_train, marker=\"o\", label=\"Train RMSE\")\n",
    "    plt.plot(Ms, rmse_val,   marker=\"o\", label=\"Validation RMSE\")\n",
    "    plt.axhline(fopt_val_rmse, linestyle=\"--\", label=\"Validation RMSE of f_opt\")\n",
    "    plt.xlabel(\"Polynomial degree M\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"RMSE vs model capacity M\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTDIR}/{filename}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4de4ef0-321f-49c1-a8f4-b90cab0618b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmse_vs_log_lambda(log10_lams, rmse_train, rmse_val, filename):\n",
    "    \"\"\"\n",
    "    Plot train/validation RMSE vs log10(λ) for ridge at M=8.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(log10_lams, rmse_train, marker=\"o\", label=\"Train RMSE\")\n",
    "    plt.plot(log10_lams, rmse_val,   marker=\"o\", label=\"Validation RMSE\")\n",
    "    plt.xlabel(\"log10(lambda)\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Ridge (M=8): RMSE vs log10(lambda)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTDIR}/{filename}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6282342-fd0a-4ac4-8ea4-d953a3100731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge(Phi_std, t, lam):\n",
    "    \"\"\"\n",
    "    Ridge closed-form with standardized features (bias column kept unscaled):\n",
    "      w = (Φ^T Φ + λ I)^{-1} Φ^T t\n",
    "    \"\"\"\n",
    "    A = Phi_std.T @ Phi_std + lam * np.eye(Phi_std.shape[1])\n",
    "    b = Phi_std.T @ t\n",
    "    w = np.linalg.solve(A, b)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43d34938-7b1f-443a-903e-bd756c8c7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_except_bias(Phi_train, *Phi_others):\n",
    "    \"\"\"\n",
    "    Standardize all columns except the first (bias) using training statistics.\n",
    "    Returns:\n",
    "      scaler, Phi_train_std, [Phi_other1_std, Phi_other2_std, ...]\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    # Split bias and features:\n",
    "    bias_tr  = Phi_train[:, [0]]\n",
    "    feats_tr = Phi_train[:, 1:]\n",
    "    feats_tr_std = scaler.fit_transform(feats_tr)\n",
    "    Phi_tr_std = np.hstack([bias_tr, feats_tr_std])\n",
    "\n",
    "    others_std = []\n",
    "    for Phi in Phi_others:\n",
    "        bias  = Phi[:, [0]]\n",
    "        feats = Phi[:, 1:]\n",
    "        feats_std = scaler.transform(feats)\n",
    "        others_std.append(np.hstack([bias, feats_std]))\n",
    "\n",
    "    return scaler, Phi_tr_std, others_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b023ad45-de2c-4540-a77f-22de3a8c8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(best_model, x_tr, t_tr, x_te, t_te, scaler_M8=None, M_ridge=8):\n",
    "    \"\"\"\n",
    "    Evaluate test RMSE for the chosen best model.\n",
    "    best_model = dict with keys:\n",
    "      - 'kind': 'ols' or 'ridge'\n",
    "      - 'M': degree\n",
    "      - 'lambda': float (np.nan for OLS)\n",
    "      - 'w': parameter vector\n",
    "    \"\"\"\n",
    "    if best_model[\"kind\"] == \"ols\":\n",
    "        M = best_model[\"M\"]\n",
    "        Phi_te = design_matrix(x_te, M)\n",
    "        # Fit on train again (already done, but refit to be explicit and clean)\n",
    "        w = fit_least_squares(design_matrix(x_tr, M), t_tr)\n",
    "        y_te = Phi_te @ w\n",
    "    else:\n",
    "        # Ridge for M=8 with standardization\n",
    "        M = M_ridge\n",
    "        Phi_te = design_matrix(x_te, M)\n",
    "        # Standardize test with the scaler fit on train\n",
    "        bias  = Phi_te[:, [0]]\n",
    "        feats = Phi_te[:, 1:]\n",
    "        feats_std = scaler_M8.transform(feats)\n",
    "        Phi_te_std = np.hstack([bias, feats_std])\n",
    "        # Refit w on (standardized) train for the selected lambda\n",
    "        lam = best_model[\"lambda\"]\n",
    "        # Recompute standardized Φ_train for reproducibility:\n",
    "        Phi_tr = design_matrix(x_tr, M)\n",
    "        _, Phi_tr_std, _ = standardize_except_bias(Phi_tr)\n",
    "        w = fit_ridge(Phi_tr_std, t_tr, lam)\n",
    "        y_te = Phi_te_std @ w\n",
    "\n",
    "    return rmse(t_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f71dd476-463f-4a7c-8c09-7ae0e67769d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # -------------------------\n",
    "    # Steps 1–2: data + seed\n",
    "    # -------------------------\n",
    "    x_tr, t_tr, x_va, t_va, x_te, t_te = generate_sets(SEED, N_TRAIN, N_VALID, N_TEST)\n",
    "    x_grid = np.linspace(0.0, 1.0, 400)  # dense grid for smooth curves\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # Steps 3–5: unregularized fits for M=1..8\n",
    "    # -------------------------------------------\n",
    "    Ms = list(range(1, N_TRAIN))  # M = 1..8 when N_TRAIN=9\n",
    "    rmse_train_list = []\n",
    "    rmse_valid_list = []\n",
    "    results_rows = []  # will store summary rows for CSV\n",
    "\n",
    "    best_model = None\n",
    "    best_val_rmse = np.inf\n",
    "\n",
    "    for M in Ms:\n",
    "        # Design matrices\n",
    "        Phi_tr = design_matrix(x_tr, M)\n",
    "        Phi_va = design_matrix(x_va, M)\n",
    "        Phi_g  = design_matrix(x_grid, M)\n",
    "\n",
    "        # Step 4: OLS fit on training\n",
    "        w = fit_least_squares(Phi_tr, t_tr)\n",
    "\n",
    "        # Predictions\n",
    "        y_tr = Phi_tr @ w\n",
    "        y_va = Phi_va @ w\n",
    "        y_g  = Phi_g  @ w\n",
    "\n",
    "        # RMSE\n",
    "        r_tr = rmse(t_tr, y_tr)\n",
    "        r_va = rmse(t_va, y_va)\n",
    "        rmse_train_list.append(r_tr)\n",
    "        rmse_valid_list.append(r_va)\n",
    "\n",
    "        # Save curve figure (Step 5: per-M figure)\n",
    "        plot_model_vs_truth(\n",
    "            x_grid=x_grid, y_model=y_g,\n",
    "            x_tr=x_tr, t_tr=t_tr,\n",
    "            x_va=x_va, t_va=t_va,\n",
    "            title=f\"Unregularized polynomial fit (M={M})\",\n",
    "            filename=f\"curve_unreg_M{M}.png\"\n",
    "        )\n",
    "\n",
    "        # Track best model by validation RMSE\n",
    "        if r_va < best_val_rmse:\n",
    "            best_val_rmse = r_va\n",
    "            best_model = {\n",
    "                \"kind\": \"ols\",\n",
    "                \"M\": M,\n",
    "                \"lambda\": np.nan,\n",
    "                \"w\": w.copy(),\n",
    "                \"val_rmse\": r_va\n",
    "            }\n",
    "\n",
    "        # For CSV summary\n",
    "        results_rows.append({\n",
    "            \"model_kind\": \"OLS\",\n",
    "            \"M\": M,\n",
    "            \"lambda\": np.nan,\n",
    "            \"train_RMSE\": r_tr,\n",
    "            \"valid_RMSE\": r_va\n",
    "        })\n",
    "\n",
    "    # Step 5: RMSE vs M with f_opt validation RMSE line\n",
    "    fopt_va_rmse = rmse(t_va, f_opt(x_va))\n",
    "    plot_rmse_vs_M(Ms, rmse_train_list, rmse_valid_list, fopt_va_rmse, \"rmse_vs_M.png\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Step 6: Ridge for M=8 (N-1), standardize features, sweep λ\n",
    "    # ---------------------------------------------------------\n",
    "    M_ridge = N_TRAIN - 1  # 8\n",
    "    Phi_tr8 = design_matrix(x_tr, M_ridge)\n",
    "    Phi_va8 = design_matrix(x_va, M_ridge)\n",
    "    Phi_g8  = design_matrix(x_grid, M_ridge)\n",
    "\n",
    "    # Standardize (train stats), keep bias unscaled\n",
    "    scaler_M8, Phi_tr8_std, [Phi_va8_std, Phi_g8_std] = standardize_except_bias(Phi_tr8, Phi_va8, Phi_g8)\n",
    "\n",
    "    exps = np.arange(-14, 3, dtype=int)  # -14 .. +2 inclusive\n",
    "    lambdas = (10.0 ** exps).astype(float)\n",
    "    log10_lams = exps.astype(float)\n",
    "\n",
    "    rmse_tr_ridge = []\n",
    "    rmse_va_ridge = []\n",
    "    ridge_param_vectors = {}\n",
    "\n",
    "    for lam in lambdas:\n",
    "        w_lam = fit_ridge(Phi_tr8_std, t_tr, lam)\n",
    "        y_tr8 = Phi_tr8_std @ w_lam\n",
    "        y_va8 = Phi_va8_std @ w_lam\n",
    "\n",
    "        rtr = rmse(t_tr, y_tr8)\n",
    "        rva = rmse(t_va, y_va8)\n",
    "        rmse_tr_ridge.append(rtr)\n",
    "        rmse_va_ridge.append(rva)\n",
    "        ridge_param_vectors[lam] = w_lam.copy()\n",
    "\n",
    "        # Track best model (now including ridge candidates)\n",
    "        if rva < best_val_rmse:\n",
    "            best_val_rmse = rva\n",
    "            best_model = {\n",
    "                \"kind\": \"ridge\",\n",
    "                \"M\": M_ridge,\n",
    "                \"lambda\": float(lam),\n",
    "                \"w\": w_lam.copy(),\n",
    "                \"val_rmse\": rva\n",
    "            }\n",
    "\n",
    "        # For CSV\n",
    "        results_rows.append({\n",
    "            \"model_kind\": \"Ridge\",\n",
    "            \"M\": M_ridge,\n",
    "            \"lambda\": float(lam),\n",
    "            \"train_RMSE\": rtr,\n",
    "            \"valid_RMSE\": rva\n",
    "        })\n",
    "\n",
    "    # Plot RMSE vs log10(lambda) (Step 6)\n",
    "    plot_rmse_vs_log_lambda(log10_lams, rmse_tr_ridge, rmse_va_ridge, \"ridge_rmse_vs_log10lambda.png\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step 7: pick λ1 (overfit), λ2 (best), λ3 (underfit) and plot\n",
    "    # ----------------------------------------------------------------\n",
    "    # λ2 = argmin validation RMSE in the ridge sweep\n",
    "    idx_best = int(np.argmin(rmse_va_ridge))\n",
    "    lam2 = float(lambdas[idx_best])     # \"optimal region\"\n",
    "    lam1 = float(lambdas[0])            # smallest λ => overfitting region\n",
    "    lam3 = float(lambdas[-1])           # largest  λ => underfitting region\n",
    "\n",
    "    for lam in [lam1, lam2, lam3]:\n",
    "        w_lam = ridge_param_vectors[lam]\n",
    "        y_grid = Phi_g8_std @ w_lam\n",
    "        plot_model_vs_truth(\n",
    "            x_grid=x_grid, y_model=y_grid,\n",
    "            x_tr=x_tr, t_tr=t_tr,\n",
    "            x_va=x_va, t_va=t_va,\n",
    "            title=f\"Ridge (M=8) — lambda={lam:.1e}\",\n",
    "            filename=f\"curve_ridge_lambda_{lam:.1e}.png\".replace(\"+\", \"\")\n",
    "        )\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Step 8: Evaluate TEST RMSE for the selected best model\n",
    "    # -----------------------------------------------------------\n",
    "    test_rmse = evaluate_test(best_model, x_tr, t_tr, x_te, t_te, scaler_M8=scaler_M8, M_ridge=M_ridge)\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eb6c80-40df-4c32-8bad-7160961791d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
