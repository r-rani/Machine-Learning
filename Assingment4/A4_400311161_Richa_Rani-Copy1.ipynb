{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b742fee6-c722-4c1c-adc2-0aca1eeef3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3a5e30-c74e-406d-b22f-01bd6f0feef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1131\n",
    "lr= 0.01\n",
    "#input = 784 pixels, 156 for 2 hidden layers, output= 10 different types of clothing items\n",
    "layers= [784, 156, 156, 10]\n",
    "batch_size= 256\n",
    "lam = 0.0018738\n",
    "early_stopping = 10\n",
    "max_epochs= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46aeba07-9f30-478b-ad3b-6f48add4f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error because file showing as non existent \n",
    "import os\n",
    "# Create data folder if it doesn't exist\n",
    "os.makedirs('./data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08010559-0d28-4c4a-a322-1cc311c85dc6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#define number of classes\n",
    "num_classes=10\n",
    "\n",
    "#load fasion MNUST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n",
    "\n",
    "print(train_dataset.data.shape, test_dataset.data.shape)\n",
    "\n",
    "# Prepare the data as numpy arrays\n",
    "X_train = train_dataset.data.numpy().reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "Y_train = train_dataset.targets.numpy()\n",
    "\n",
    "X_test = test_dataset.data.numpy().reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "Y_test = test_dataset.targets.numpy()\n",
    "\n",
    "# Split the training set into train and validation sets (80% / 20%)\n",
    "validation_size = int(0.2 * X_train.shape[0])\n",
    "X_validation, Y_validation = X_train[:validation_size], Y_train[:validation_size]\n",
    "X_train, Y_train = X_train[validation_size:], Y_train[validation_size:]\n",
    "\n",
    "# Save original labels before one-hot encoding\n",
    "Y_train_orig = Y_train\n",
    "Y_validation_orig = Y_validation\n",
    "Y_test_orig = Y_test\n",
    "\n",
    "# Convert labels to one-hot encoding for multi-class classification\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "Y_train = one_hot_encode(Y_train, num_classes)\n",
    "Y_validation = one_hot_encode(Y_validation, num_classes)\n",
    "Y_test = one_hot_encode(Y_test, num_classes)\n",
    "\n",
    "# Standardizing the data\n",
    "\n",
    "# Calculate the mean and standard deviation of the training features\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "X_train_std[X_train_std == 0] = 1  # To avoid division by zero\n",
    "\n",
    "# Standardize all three subsets of data\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_validation = (X_validation - X_train_mean) / X_train_std\n",
    "X_test = (X_test - X_train_mean) / X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25d050d-d5ef-41d8-a37d-1f0d3e407d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_uniform_1_over_sqrt_m(rng, m, n, dtype=np.float32):\n",
    "    # Unif(-1/sqrt(m), 1/sqrt(m))\n",
    "    bound = 1.0 / np.sqrt(m)\n",
    "    W = rng.uniform(-bound, bound, size=(m, n)).astype(dtype)\n",
    "    b = np.zeros((n,), dtype=dtype)  # biases = 0\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d340ee6-7810-4ffd-87e9-13e4974272b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_uniform_sqrt6_over_m_plus_n(rng, m, n, dtype=np.float32):\n",
    "    # Unif(-sqrt(6/(m+n)), sqrt(6/(m+n)))\n",
    "    bound = np.sqrt(6.0 / (m + n))\n",
    "    W = rng.uniform(-bound, bound, size=(m, n)).astype(dtype)\n",
    "    b = np.zeros((n,), dtype=dtype)  # biases = 0\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2a5898-939f-4e18-9033-ab6314c998a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):  # hidden activation\n",
    "    return np.maximum(x, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d97039-3dbc-4a05-9ccc-2f8fd8b00c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(z):\n",
    "    return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e70f285-867d-4917-ada5-ea594b9e4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z = z - z.max(axis=1, keepdims=True)# stability\n",
    "    e = np.exp(z)\n",
    "    return e / e.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c848c0-bdbb-4f56-9047-9f5177b7e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_onehot(p, t):\n",
    "    return -np.mean(np.sum(t * np.log(p + 1e-12), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71ad74ab-2a1c-48a3-8b0b-a50efed7d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(W_list):\n",
    "    \"\"\"\n",
    "    Sum of squares of all weights in all layers.\n",
    "    L2 = sum_l ||W^(l)||_F^2   (Frobenius norm squared)\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    for W in W_list:        # loop over each weight matrix\n",
    "        total += np.sum(W * W)\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d329ba6-7223-4fb2-83a2-7bb252a97f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_params(layers, init_name, seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    W_list = []\n",
    "    b_list = []\n",
    "    \n",
    "    for i in range(len(layers) - 1):\n",
    "        m = layers[i]     # fan-in\n",
    "        n = layers[i+1]   # fan-out\n",
    "\n",
    "        if init_name == \"uniform_1_sqrt_m\":\n",
    "            W, b = init_uniform_1_over_sqrt_m(rng, m, n)\n",
    "        elif init_name == \"uniform_sqrt6_m_plus_n\":\n",
    "            W, b = init_uniform_sqrt6_over_m_plus_n(rng, m, n)\n",
    "\n",
    "        W_list.append(W)\n",
    "        b_list.append(b)\n",
    "\n",
    "    return  W_list, b_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df61a818-bd47-440f-865f-88c75f91bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W, B = build_params(layers, \"uniform_1_sqrt_m\", seed)\n",
    "#print(W)\n",
    "#print(len(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2a3dd92-0aca-4d4a-983e-ebcc34681aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W_list, b_list):\n",
    "    \"\"\"\n",
    "    Forward pass through an L-layer MLP (ReLU hidden, softmax output).\n",
    "\n",
    "    X      : (N, d_in)   mini-batch\n",
    "    W_list : [W1,...,WL]  (d_in x h1, h1 x h2, ..., h_{L-1} x K)\n",
    "    b_list : [b1,...,bL]  (h1,), ..., (K,)\n",
    "\n",
    "    Returns:\n",
    "      P      : (N, K) probabilities\n",
    "      caches : list of (A_prev, Z) for each layer (needed in backprop)\n",
    "    \"\"\"\n",
    "    A = X                  # current activations (A^0 = X)\n",
    "    caches = []            # to store (A_{ℓ-1}, Z_ℓ)\n",
    "\n",
    "    # ----- hidden layers: 1 .. L-1 -----\n",
    "    for W, b in zip(W_list[:-1], b_list[:-1]):\n",
    "        Z = A @ W + b      # (N, n_ℓ)\n",
    "        caches.append((A, Z))\n",
    "        A = relu(Z)\n",
    "\n",
    "    # ----- output layer: L -----\n",
    "    ZL = A @ W_list[-1] + b_list[-1]   # (N, K)\n",
    "    P = softmax(ZL)                    # (N, K)\n",
    "    caches.append((A, ZL))             # last hidden A_{L-1}, Z_L\n",
    "\n",
    "    return P, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1210d1e1-f295-499d-aa88-ef4b0009ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(P, T, W_list, b_list, caches, lam=0.0):\n",
    "    \"\"\"\n",
    "    Back-prop for the same L-layer MLP.\n",
    "\n",
    "    P      : (N, K) probs from forward_pass\n",
    "    T      : (N, K) one-hot targets\n",
    "    W_list : [W1,...,WL]\n",
    "    b_list : [b1,...,bL]\n",
    "    caches : list of (A_prev, Z) from forward_pass\n",
    "    lam    : L2 weight decay coefficient\n",
    "\n",
    "    Returns:\n",
    "      loss    : CE + L2\n",
    "      grads_W : list of dJ/dW for each W\n",
    "      grads_b : list of dJ/db for each b\n",
    "    \"\"\"\n",
    "    L = len(W_list)             # number of layers\n",
    "    N = T.shape[0]              # batch size\n",
    "\n",
    "    # ----- loss = CE + λ * ||W||^2 -----\n",
    "    ce = cross_entropy_onehot(P, T)\n",
    "    l2 = l2_penalty(W_list)\n",
    "    loss = ce + lam * l2\n",
    "\n",
    "    # lists of gradients (Python lists, *not* numpy arrays)\n",
    "    grads_W = [None] * L\n",
    "    grads_b = [None] * L\n",
    "\n",
    "    # ===== output layer gradient (layer L) =====\n",
    "    # dJ/dZ_L = (P - T) / N  (for mini-batch size N)\n",
    "    dZ = (P - T) / N          # (N, K)\n",
    "\n",
    "    A_prev, ZL = caches[-1]   # last hidden A_{L-1}, Z_L\n",
    "\n",
    "    grads_W[-1] = A_prev.T @ dZ + 2 * lam * W_list[-1]  # (h_{L-1}, K)\n",
    "    grads_b[-1] = dZ.sum(axis=0)                        # (K,)\n",
    "\n",
    "    # backprop to previous activations\n",
    "    dA = dZ @ W_list[-1].T    # (N, h_{L-1})\n",
    "\n",
    "    # ===== hidden layers: L-1 down to 1 =====\n",
    "    for i in range(L-2, -1, -1):\n",
    "        A_prev, Z = caches[i]        # A_{i-1}, Z_i\n",
    "        dZ = dA * relu_grad(Z)       # ⊙ g'(Z_i)\n",
    "        grads_W[i] = A_prev.T @ dZ + 2 * lam * W_list[i]\n",
    "        grads_b[i] = dZ.sum(axis=0)\n",
    "        dA = dZ @ W_list[i].T        # backprop to A_{i-1}\n",
    "\n",
    "    return loss, grads_W, grads_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43cf8e2e-ea74-4de7-ba14-1910c0207f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_loss(X, T, W_list, b_list, lam, eval_batch_size=2048):\n",
    "    \"\"\"\n",
    "    Compute full loss (cross-entropy + L2) on a dataset, in chunks.\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    total_ce = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for start in range(0, N, eval_batch_size):\n",
    "        xb = X[start:start + eval_batch_size]\n",
    "        tb = T[start:start + eval_batch_size]\n",
    "        P, _ = forward_pass(xb, W_list, b_list)\n",
    "        ce = cross_entropy_onehot(P, tb)\n",
    "        total_ce += ce * xb.shape[0]\n",
    "        count += xb.shape[0]\n",
    "\n",
    "    ce_mean = total_ce / count\n",
    "    l2 = sum((W**2).sum() for W in W_list)\n",
    "    return ce_mean + lam * l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "641c2f35-b25d-41b6-ae9c-49d3a56b580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes(X, W_list, b_list, eval_batch_size=2048):\n",
    "    N = X.shape[0]\n",
    "    preds = []\n",
    "    for start in range(0, N, eval_batch_size):\n",
    "        xb = X[start:start + eval_batch_size]\n",
    "        P, _ = forward_pass(xb, W_list, b_list)\n",
    "        preds.append(np.argmax(P, axis=1))\n",
    "    return np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903c329-3e80-45cd-8bfd-c3138fdfc84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Tiny fake dataset: 5 samples, 8 features, 3 classes\n",
    "X_fake = np.random.rand(5, 8).astype(np.float32)\n",
    "T_fake = np.eye(3)[np.random.randint(0, 3, size=5)]\n",
    "\n",
    "# Tiny network: 8 -> 4 -> 4 -> 3\n",
    "layers_test = [8, 4, 4, 3]\n",
    "W_list, b_list = build_params(layers_test, \"uniform_1_sqrt_m\", seed=123)\n",
    "\n",
    "# Forward + backward once\n",
    "P, caches = forward_pass(X_fake, W_list, b_list)\n",
    "loss, gW, gB = backward_pass(P, T_fake, W_list, b_list, caches, lam=0.01)\n",
    "\n",
    "print(\"Sanity loss:\", loss)\n",
    "print(\"W shapes:\", [W.shape for W in W_list])\n",
    "print(\"grad W shapes:\", [G.shape for G in gW])\n",
    "print(\"grad b shapes:\", [g.shape for g in gB])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d3abb44-9175-4652-90dc-55d79bfb0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_model(X_train, Y_train,\n",
    "                    X_val,   Y_val,\n",
    "                    init_name, seed_init):\n",
    "    \"\"\"\n",
    "    Train ONE network with:\n",
    "      - mini-batch gradient descent\n",
    "      - L2 weight decay\n",
    "      - early stopping on validation loss\n",
    "    Returns:\n",
    "      W_list, b_list, train_losses, val_losses\n",
    "    \"\"\"\n",
    "\n",
    "    # initialise parameters\n",
    "    params = build_params(layers, init_name, seed_init)\n",
    "    W_list, b_list = params[\"W\"], params[\"b\"]\n",
    "\n",
    "    rng = np.random.default_rng(seed_init)\n",
    "    N = X_train.shape[0]\n",
    "\n",
    "    best_val = np.inf\n",
    "    best_W, best_b = None, None\n",
    "    wait = 0\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # --- shuffle indices ---\n",
    "        idx = np.arange(N)\n",
    "        rng.shuffle(idx)\n",
    "\n",
    "        epoch_loss_sum = 0.0\n",
    "        epoch_count = 0\n",
    "\n",
    "        # --- mini-batch loop ---\n",
    "        for start in range(0, N, batch_size):\n",
    "            batch_idx = idx[start:start + batch_size]\n",
    "            xb = X_train[batch_idx]\n",
    "            tb = Y_train[batch_idx]\n",
    "\n",
    "            # forward\n",
    "            P, caches = forward_pass(xb, W_list, b_list)\n",
    "\n",
    "            # backward\n",
    "            loss_batch, grads_W, grads_b = backward_pass(\n",
    "                P, tb, W_list, b_list, caches, lam=lam\n",
    "            )\n",
    "\n",
    "            # accumulate batch loss (for train loss curve)\n",
    "            bs = xb.shape[0]\n",
    "            epoch_loss_sum += loss_batch * bs\n",
    "            epoch_count += bs\n",
    "\n",
    "            # update parameters\n",
    "            for i in range(len(W_list)):\n",
    "                W_list[i] -= lr * grads_W[i]\n",
    "                b_list[i] -= lr * grads_b[i]\n",
    "\n",
    "        # --- end of epoch: compute average train loss ---\n",
    "        train_loss = epoch_loss_sum / epoch_count\n",
    "\n",
    "        # and validation loss (in chunks, smaller set)\n",
    "        val_loss = evaluate_loss(X_val, Y_val, W_list, b_list, lam)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:3d} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f}\")\n",
    "\n",
    "        # --- early stopping ---\n",
    "        if val_loss + 1e-6 < best_val:\n",
    "            best_val = val_loss\n",
    "            best_W = [W.copy() for W in W_list]\n",
    "            best_b = [b.copy() for b in b_list]\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= early_stopping:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                W_list = best_W\n",
    "                b_list = best_b\n",
    "                break\n",
    "\n",
    "    return W_list, b_list, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf895b2b-f69d-489d-8a45-666b7724aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    results = {}\n",
    "\n",
    "    # --------- Model A ---------\n",
    "    print(\"Training Model A (Uniform(-1/sqrt(m), 1/sqrt(m)))...\")\n",
    "    W_A, b_A, train_A, val_A = train_one_model(\n",
    "        X_train, Y_train,\n",
    "        X_validation, Y_validation,\n",
    "        init_name=\"uniform_1_sqrt_m\",\n",
    "        seed_init=seed,\n",
    "    )\n",
    "\n",
    "    y_train_pred_A = predict_classes(X_train, W_A, b_A)\n",
    "    y_test_pred_A  = predict_classes(X_test,  W_A, b_A)\n",
    "\n",
    "    train_err_A = 100 * (1 - np.mean(y_train_pred_A == Y_train_orig))\n",
    "    test_err_A  = 100 * (1 - np.mean(y_test_pred_A  == Y_test_orig))\n",
    "\n",
    "    results[\"A\"] = {\n",
    "        \"W\": W_A, \"b\": b_A,\n",
    "        \"train_losses\": train_A,\n",
    "        \"val_losses\": val_A,\n",
    "        \"train_err\": train_err_A,\n",
    "        \"test_err\": test_err_A,\n",
    "    }\n",
    "\n",
    "    # --------- Model B ---------\n",
    "    print(\"\\nTraining Model B (Uniform(-sqrt(6/(m+n)), sqrt(6/(m+n)))...)\")\n",
    "    seed_B = seed + 1\n",
    "    W_B, b_B, train_B, val_B = train_one_model(\n",
    "        X_train, Y_train,\n",
    "        X_validation, Y_validation,\n",
    "        init_name=\"uniform_sqrt6_m_plus_n\",\n",
    "        seed_init=seed_B,\n",
    "    )\n",
    "\n",
    "    y_train_pred_B = predict_classes(X_train, W_B, b_B)\n",
    "    y_test_pred_B  = predict_classes(X_test,  W_B, b_B)\n",
    "\n",
    "    train_err_B = 100 * (1 - np.mean(y_train_pred_B == Y_train_orig))\n",
    "    test_err_B  = 100 * (1 - np.mean(y_test_pred_B  == Y_test_orig))\n",
    "\n",
    "    results[\"B\"] = {\n",
    "        \"W\": W_B, \"b\": b_B,\n",
    "        \"train_losses\": train_B,\n",
    "        \"val_losses\": val_B,\n",
    "        \"train_err\": train_err_B,\n",
    "        \"test_err\": test_err_B,\n",
    "    }\n",
    "\n",
    "    # --------- print misclassification errors ---------\n",
    "    print(\"\\n=== Misclassification Errors ===\")\n",
    "    print(\"Model A: Uniform(-1/sqrt(m), 1/sqrt(m))\")\n",
    "    print(f\"  Train error: {train_err_A:.2f}%\")\n",
    "    print(f\"  Test  error: {test_err_A:.2f}%\\n\")\n",
    "\n",
    "    print(\"Model B: Uniform(-sqrt(6/(m+n)), sqrt(6/(m+n)))\")\n",
    "    print(f\"  Train error: {train_err_B:.2f}%\")\n",
    "    print(f\"  Test  error: {test_err_B:.2f}%\\n\")\n",
    "\n",
    "    # --------- plots ---------\n",
    "    epochs_A = range(1, len(train_A) + 1)\n",
    "    epochs_B = range(1, len(train_B) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Model A\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epochs_A, train_A, label=\"Train loss (Model A)\")\n",
    "    plt.plot(epochs_A, val_A,   label=\"Validation loss (Model A)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Model A: Uniform(-1/sqrt(m), 1/sqrt(m))\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Model B\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs_B, train_B, label=\"Train loss (Model B)\")\n",
    "    plt.plot(epochs_B, val_B,   label=\"Validation loss (Model B)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Model B: Uniform(-sqrt(6/(m+n)), sqrt(6/(m+n)))\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224bf68-d98d-478f-b463-54a9ac9166b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURE NUMPY SANITY TEST: no Fashion-MNIST involved\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# tiny fake dataset: 5 samples, 784 features, 10 classes\n",
    "X_fake = np.random.randn(5, 784).astype(np.float32)\n",
    "T_fake = np.eye(10)[np.random.randint(0, 10, size=5)]   # random one-hot labels\n",
    "\n",
    "# tiny 2-hidden-layer net: 784 -> 4 -> 3 -> 10\n",
    "layers_test = [784, 4, 3, 10]\n",
    "\n",
    "# simple random init (no fancy formulas, just to test)\n",
    "W_list = []\n",
    "b_list = []\n",
    "for m, n in zip(layers_test[:-1], layers_test[1:]):\n",
    "    W_list.append(np.random.randn(m, n).astype(np.float32) * 0.1)\n",
    "    b_list.append(np.zeros(n, dtype=np.float32))\n",
    "\n",
    "# run forward and backward once\n",
    "P, caches = forward_pass(X_fake, W_list, b_list)\n",
    "loss, gW, gB = backward_pass(P, T_fake, W_list, b_list, caches, lam=0.01)\n",
    "\n",
    "print(\"Sanity loss:\", loss)\n",
    "print(\"grad W shapes:\", [g.shape for g in gW])\n",
    "print(\"grad b shapes:\", [g.shape for g in gB])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd48075-f2bf-41fd-9420-c7c25221416f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5353b1-8c42-42ec-a6b2-bcdfb5dd0300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
