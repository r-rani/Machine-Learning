{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cda72ef-fc99-4b0d-b4c0-c3d12ca23ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d20561d-7ed3-4bf6-bbe9-6541af11b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1131\n",
    "lr= 0.01\n",
    "#input = 784 pixels, 156 for 2 hidden layers, output= 10 different types of clothing items\n",
    "layers= [784, 156, 156, 10]\n",
    "batch_size= 256 #minibatch\n",
    "lam = 0.0018738\n",
    "early_stopping = 10 #how many times without better results until you stop\n",
    "max_epochs= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5db40304-9272-4566-b422-6f0748197e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#define number of classes\n",
    "num_classes=10\n",
    "\n",
    "#load fasion MNUST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n",
    "\n",
    "print(train_dataset.data.shape, test_dataset.data.shape)\n",
    "\n",
    "# Prepare the data as numpy arrays\n",
    "X_train = train_dataset.data.numpy().reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "Y_train = train_dataset.targets.numpy()\n",
    "\n",
    "X_test = test_dataset.data.numpy().reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "Y_test = test_dataset.targets.numpy()\n",
    "\n",
    "# Split the training set into train and validation sets (80% / 20%)\n",
    "validation_size = int(0.2 * X_train.shape[0])\n",
    "X_validation, Y_validation = X_train[:validation_size], Y_train[:validation_size]\n",
    "X_train, Y_train = X_train[validation_size:], Y_train[validation_size:]\n",
    "\n",
    "# Save original labels before one-hot encoding\n",
    "Y_train_orig = Y_train\n",
    "Y_validation_orig = Y_validation\n",
    "Y_test_orig = Y_test\n",
    "\n",
    "# Convert labels to one-hot encoding for multi-class classification\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "Y_train = one_hot_encode(Y_train, num_classes)\n",
    "Y_validation = one_hot_encode(Y_validation, num_classes)\n",
    "Y_test = one_hot_encode(Y_test, num_classes)\n",
    "\n",
    "# Standardizing the data\n",
    "\n",
    "# Calculate the mean and standard deviation of the training features\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "X_train_std[X_train_std == 0] = 1  # To avoid division by zero\n",
    "\n",
    "# Standardize all three subsets of data\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_validation = (X_validation - X_train_mean) / X_train_std\n",
    "X_test = (X_test - X_train_mean) / X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6272f15d-b246-41f5-9ddd-210621ee166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations for W and b\n",
    "\n",
    "def init_uniform_1_over_sqrt_m(rng, m, n, dtype=np.float32):\n",
    "    #W_ij ~ Unif(-1/sqrt(m), 1/sqrt(m)),  m = fan-in\n",
    "    bound = 1.0 / np.sqrt(m) #the boundareis\n",
    "    W = rng.uniform(-bound, bound, size=(m, n)).astype(dtype) #the uniform distribute between -bound adn bound\n",
    "    b = np.zeros((n,), dtype=dtype)#bias is 0 \n",
    "    return W, b\n",
    "\n",
    "\n",
    "def init_uniform_sqrt6_over_m_plus_n(rng, m, n, dtype=np.float32):\n",
    "    #W_ij ~ Unif(-sqrt(6/(m+n)), sqrt(6/(m+n)))\n",
    "    bound = np.sqrt(6.0 / (m + n))\n",
    "    W = rng.uniform(-bound, bound, size=(m, n)).astype(dtype)\n",
    "    b = np.zeros((n,), dtype=dtype)\n",
    "    return W, b\n",
    "\n",
    "\n",
    "def build_params(layers, init_name, seed):\n",
    "    #layers = [n_in, n_h1, ..., n_out], init is the sqt1 or sqrt6, seed is 1311\n",
    "    #returns:W_list: [W1, ..., WL],b_list: [b1, ..., bL]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    W_list= [] \n",
    "    b_list = []\n",
    "\n",
    "    for i in range(len(layers) - 1):\n",
    "        m = layers[i] \n",
    "        n = layers[i + 1] \n",
    "\n",
    "        if init_name == \"uniform_1_sqrt_m\": \n",
    "            W, b = init_uniform_1_over_sqrt_m(rng, m, n) \n",
    "        elif init_name == \"uniform_sqrt6_m_plus_n\":\n",
    "            W, b = init_uniform_sqrt6_over_m_plus_n(rng, m, n)\n",
    "\n",
    "        W_list.append(W)\n",
    "        b_list.append(b)\n",
    "\n",
    "    return W_list, b_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7bbe1a-fdf4-4049-87a3-ab514f42bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation functions \n",
    "def relu(z): \n",
    "    return np.maximum(z, 0.0)\n",
    "\n",
    "def relu_grad(z):\n",
    "    # derivative of ReLU\n",
    "    return (z > 0).astype(float) #fiecewise function from slides\n",
    "\n",
    "def softmax(z): \n",
    "    # softamx e^z/sum(e^z)\n",
    "    z = z - z.max(axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / e.sum(axis=1, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fadd1a-8842-4c13-9f93-25897450fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss functions \n",
    "def cross_entropy_onehot(p, t):\n",
    "    # p, t : (N, K)\n",
    "    return -np.mean(np.sum(t * np.log(p + 1e-12), axis=1))\n",
    "\n",
    "def l2_penalty(W_list):\n",
    "    #sum over all layers l of ||W^(l)||_F^2\n",
    "\n",
    "    total = 0.0\n",
    "    for W in W_list:\n",
    "        total += np.sum(W * W)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf0f7676-47a1-444c-98e8-2f4ff5b33e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward / Backward propagation\n",
    "\n",
    "def forward_pass(X, W_list, b_list):\n",
    "    #Hidden layers: ReLU\n",
    "    #Output layer: softmax\n",
    "    A = X  # current activations\n",
    "    caches = [] # store (A_prev, Z) for each layer\n",
    "\n",
    "    # hidden layers: 0 to L-2. zip will iterate through both till -1 or the last elemet\n",
    "    for W, b in zip(W_list[:-1], b_list[:-1]):\n",
    "        Z = np.dot(A, W) + b # (N, n_L)\n",
    "        caches.append((A, Z))# save A^{(l-1)} and Z^{(l)}\n",
    "        A = relu(Z)# activation for next layer\n",
    "\n",
    "    # output layer \n",
    "    ZL = np.dot(A , W_list[-1]) + b_list[-1] # (N, K)\n",
    "    P  = softmax(ZL) # (N, K) activiation \n",
    "    caches.append((A, ZL)) # last hidden A, output Z\n",
    "\n",
    "    return P, caches\n",
    "\n",
    "\n",
    "def backward_pass(P, T, W_list, b_list, caches, lam=0.0):\n",
    "\n",
    "\n",
    "    #P predicted probs\n",
    "    #T one-hot targets\n",
    "\n",
    "    #Returns\n",
    "    #loss CE + L2\n",
    "    #grads_W: list dJ/dW_l same shapes as W_list\n",
    "    #grads_b: list dJ/db_l same shapes as b_list\n",
    "\n",
    "    L = len(W_list)#number of weight layers\n",
    "    N = T.shape[0]#batch size\n",
    "\n",
    "    # loss = CE + lambda * sum ||W||^2 \n",
    "    ce = cross_entropy_onehot(P, T)\n",
    "    l2 = l2_penalty(W_list)\n",
    "    loss = ce + lam * l2\n",
    "    \n",
    "    #empty gradient containers\n",
    "    grads_W = np.zeros(L)\n",
    "    grads_b = np.zeros(L)\n",
    "\n",
    "    # output layer \n",
    "    # derivative dJ/dZ^L = (P - T)/N  (softmax + CE)\n",
    "    dZ = (P - T) / N\n",
    "    A_prev, ZL = caches[-1] # last hidden A, output Z\n",
    "\n",
    "    grads_W[-1] = np.dot(A_prev.T , dZ) + 2 * lam * W_list[-1] # (n_{L-1}, K)\n",
    "    grads_b[-1] = dZ.sum(axis=0) \n",
    "\n",
    "    dA = np.dot(dZ, W_list[-1].T) # backprop to A^{L-1}\n",
    "\n",
    "    # hidden layers: L-1 to 1\n",
    "    for i in range(L - 2, -1, -1):\n",
    "        A_prev, Z = caches[i] # A^{(l-1)}, Z^{(l)}\n",
    "        dZ = dA * relu_grad(Z) \n",
    "        #grads\n",
    "        grads_W[i] = np.dot(A_prev.T, dZ) + 2 * lam * W_list[i]\n",
    "        grads_b[i] = dZ.sum(axis=0)\n",
    "\n",
    "        dA = np.dot(dZ, W_list[i].T)# backprop to previous layer\n",
    "\n",
    "    return loss, grads_W, grads_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0b6a58d-c33f-47da-b1d3-2284701484ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and predict class \n",
    "#batch size is 204y for evaluation for fast bc were not training here \n",
    "\n",
    "def evaluate_loss(X, T, W_list, b_list, lam, eval_batch_size=2048):\n",
    "    \n",
    "    #Compute CE + L2 on a dataset using mini-batches\n",
    "    N = X.shape[0]\n",
    "    total_ce = 0.0 #total ce loss * batch sizes \n",
    "    count = 0 #number of samples processed so far \n",
    "\n",
    "    for start in range(0, N, eval_batch_size):\n",
    "        xb = X[start:start + eval_batch_size]\n",
    "        tb = T[start:start + eval_batch_size]\n",
    "        P, T = forward_pass(xb, W_list, b_list)\n",
    "        ce = cross_entropy_onehot(P, tb)\n",
    "        total_ce += ce * xb.shape[0]\n",
    "        count += xb.shape[0]\n",
    "\n",
    "    ce_mean = total_ce / count #avg over samples \n",
    "    l2 = l2_penalty(W_list)#sum of squares of all weights in all layers \n",
    "    return ce_mean + lam * l2\n",
    "\n",
    "\n",
    "def predict_classes(X, W_list, b_list, eval_batch_size=2048):\n",
    "    #Return predicted class indices on X.\n",
    "    N = X.shape[0]\n",
    "    preds = [] #store predictions for each mini batch in this list \n",
    "\n",
    "    for start in range(0, N, eval_batch_size): #loop over X in mini batches \n",
    "        xb = X[start:start + eval_batch_size]\n",
    "        P, T = forward_pass(xb, W_list, b_list) #frwrd pass to get prob P \n",
    "        batch_preds = np.argmax(P, axis-1) #each row is sample, shape is batch size. returns index of largest P \n",
    "        preds.append(batch_preds)\n",
    "    #concatenate all of the batch pred into one big array of (N,1)\n",
    "    return np.concatenate(preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17e29cd9-3088-4c61-a4b7-4d151e7f6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training one model \n",
    "def train_one_model(X_train, Y_train,X_val,Y_val,layers, init_name,seed, lr=0.01, lam=0.0018738, batch_size=256, max_epochs=30, early_stopping=10):\n",
    "    #Train ONE MLP with mini-batch GD + L2 + early stopping.\n",
    "    \n",
    "    rng = np.random.default_rng(seed_init)\n",
    "    N = X_train.shape[0]\n",
    "\n",
    "    # init parameters\n",
    "    W_list, b_list = build_params(layers, init_name, seed)\n",
    "\n",
    "    best_val = np.inf\n",
    "    best_W=0\n",
    "    best_b = 0\n",
    "    wait = 0\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "\n",
    "        # shuffle indices. medium article in src \n",
    "        idx = np.arange(N)\n",
    "        rng.shuffle(idx)\n",
    "\n",
    "        epoch_loss_sum = 0.0\n",
    "        epoch_count = 0\n",
    "\n",
    "        # minibatch\n",
    "        for start in range(0, N, batch_size):\n",
    "            batch_idx = idx[start:start + batch_size]\n",
    "            xb = X_train[batch_idx]\n",
    "            tb = Y_train[batch_idx]\n",
    "\n",
    "            P, caches = forward_pass(xb, W_list, b_list)\n",
    "            loss_batch, grads_W, grads_b = backward_pass(P, tb, W_list, b_list, caches, lam=lam)\n",
    "            #batch size \n",
    "            bs = xb.shape[0]\n",
    "            epoch_loss_sum += loss_batch * bs\n",
    "            epoch_count+= bs\n",
    "\n",
    "            # parameter update\n",
    "            for i in range(len(W_list)):\n",
    "                W_list[i] -= lr * grads_W[i]\n",
    "                b_list[i] -= lr * grads_b[i]\n",
    "\n",
    "        # ---- average train loss ----\n",
    "        train_loss = epoch_loss_sum / epoch_count\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # ---- validation loss ----\n",
    "        val_loss = evaluate_loss(X_val, Y_val, W_list, b_list, lam)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:3d} | train: {train_loss:.4f} | val: {val_loss:.4f}\")\n",
    "\n",
    "        # ---- early stopping ----\n",
    "        if val_loss + 1e-6 < best_val:\n",
    "            best_val = val_loss\n",
    "            best_W   = [W.copy() for W in W_list]\n",
    "            best_b   = [b.copy() for b in b_list]\n",
    "            wait     = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= early_stopping:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                W_list = best_W\n",
    "                b_list = best_b\n",
    "                break\n",
    "\n",
    "    return W_list, b_list, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28822021-bcc0-4086-af7b-7db5be0370b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model A (Unif(-1/sqrt(m), 1/sqrt(m)))...\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    results = {}\n",
    "\n",
    "    # -------- Model A --------\n",
    "    print(\"Training Model A (Unif(-1/sqrt(m), 1/sqrt(m)))...\")\n",
    "    W_A, b_A, train_A, val_A = train_one_model(\n",
    "        X_train, Y_train,\n",
    "        X_validation, Y_validation,\n",
    "        layers,\n",
    "        init_name=\"uniform_1_sqrt_m\",\n",
    "        seed_init=seed,\n",
    "        lr=lr,\n",
    "        lam=lam,\n",
    "        batch_size=batch_size,\n",
    "        max_epochs=max_epochs,\n",
    "        early_stopping=early_stopping,\n",
    "    )\n",
    "\n",
    "    y_train_pred_A = predict_classes(X_train, W_A, b_A)\n",
    "    y_test_pred_A  = predict_classes(X_test,  W_A, b_A)\n",
    "\n",
    "    train_err_A = 100 * (1.0 - np.mean(y_train_pred_A == Y_train_orig))\n",
    "    test_err_A  = 100 * (1.0 - np.mean(y_test_pred_A  == Y_test_orig))\n",
    "\n",
    "    results[\"A\"] = {\n",
    "        \"W\": W_A, \"b\": b_A,\n",
    "        \"train_losses\": train_A,\n",
    "        \"val_losses\": val_A,\n",
    "        \"train_err\": train_err_A,\n",
    "        \"test_err\": test_err_A,\n",
    "    }\n",
    "\n",
    "    # -------- Model B --------\n",
    "    print(\"\\nTraining Model B (Unif(-sqrt(6/(m+n)), sqrt(6/(m+n))))...\")\n",
    "    W_B, b_B, train_B, val_B = train_one_model(\n",
    "        X_train, Y_train,\n",
    "        X_validation, Y_validation,\n",
    "        layers,\n",
    "        init_name=\"uniform_sqrt6_m_plus_n\",\n",
    "        seed_init=seed + 1,\n",
    "        lr=lr,\n",
    "        lam=lam,\n",
    "        batch_size=batch_size,\n",
    "        max_epochs=max_epochs,\n",
    "        early_stopping=early_stopping,\n",
    "    )\n",
    "\n",
    "    y_train_pred_B = predict_classes(X_train, W_B, b_B)\n",
    "    y_test_pred_B  = predict_classes(X_test,  W_B, b_B)\n",
    "\n",
    "    train_err_B = 100 * (1.0 - np.mean(y_train_pred_B == Y_train_orig))\n",
    "    test_err_B  = 100 * (1.0 - np.mean(y_test_pred_B  == Y_test_orig))\n",
    "\n",
    "    results[\"B\"] = {\n",
    "        \"W\": W_B, \"b\": b_B,\n",
    "        \"train_losses\": train_B,\n",
    "        \"val_losses\": val_B,\n",
    "        \"train_err\": train_err_B,\n",
    "        \"test_err\": test_err_B,\n",
    "    }\n",
    "\n",
    "    # -------- Print misclassification errors --------\n",
    "    print(\"\\nMisclassification errors:\")\n",
    "    print(f\"Model A – train: {train_err_A:6.2f}% | test: {test_err_A:6.2f}%\")\n",
    "    print(f\"Model B – train: {train_err_B:6.2f}% | test: {test_err_B:6.2f}%\")\n",
    "\n",
    "    # -------- Plot CE + L2 vs epochs for both models --------\n",
    "    epochs_A = range(1, len(train_A) + 1)\n",
    "    epochs_B = range(1, len(train_B) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Model A\n",
    "    plt.plot(epochs_A, train_A, label=\"Train loss (Model A)\")\n",
    "    plt.plot(epochs_A, val_A,   label=\"Val loss (Model A)\", linestyle=\"--\")\n",
    "\n",
    "    # Model B\n",
    "    plt.plot(epochs_B, train_B, label=\"Train loss (Model B)\")\n",
    "    plt.plot(epochs_B, val_B,   label=\"Val loss (Model B)\", linestyle=\"--\")\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Cross-entropy + L2\")\n",
    "    plt.title(\"Training / validation loss for Model A & B\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run\n",
    "results = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47568cf8-6778-4a63-92c2-a758021115f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
