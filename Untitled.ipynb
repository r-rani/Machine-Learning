{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b742fee6-c722-4c1c-adc2-0aca1eeef3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "seed = 1131\n",
    "lr= 0.01\n",
    "#input = 784 pixels, 156 for 2 hidden layers, output= 10 different types of clothing items\n",
    "layers= [784, 156, 156, 10]\n",
    "batch_size= 128\n",
    "lam = 0.0018738\n",
    "early_stopping = 10\n",
    "max_epochs=100\n",
    "\n",
    "#error because file showing as non existent \n",
    "import os\n",
    "# Create data folder if it doesn't exist\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#define number of classes\n",
    "num_classes=10\n",
    "\n",
    "#load fasion MNUST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n",
    "\n",
    "print(train_dataset.data.shape, test_dataset.data.shape)\n",
    "\n",
    "# Prepare the data as numpy arrays\n",
    "X_train = train_dataset.data.numpy().reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "Y_train = train_dataset.targets.numpy()\n",
    "\n",
    "X_test = test_dataset.data.numpy().reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "Y_test = test_dataset.targets.numpy()\n",
    "\n",
    "# Split the training set into train and validation sets (80% / 20%)\n",
    "validation_size = int(0.2 * X_train.shape[0])\n",
    "X_validation, Y_validation = X_train[:validation_size], Y_train[:validation_size]\n",
    "X_train, Y_train = X_train[validation_size:], Y_train[validation_size:]\n",
    "\n",
    "# Save original labels before one-hot encoding\n",
    "Y_train_orig = Y_train\n",
    "Y_validation_orig = Y_validation\n",
    "Y_test_orig = Y_test\n",
    "\n",
    "# Convert labels to one-hot encoding for multi-class classification\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "Y_train = one_hot_encode(Y_train, num_classes)\n",
    "Y_validation = one_hot_encode(Y_validation, num_classes)\n",
    "Y_test = one_hot_encode(Y_test, num_classes)\n",
    "\n",
    "# Standardizing the data\n",
    "\n",
    "# Calculate the mean and standard deviation of the training features\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "X_train_std[X_train_std == 0] = 1  # To avoid division by zero\n",
    "\n",
    "# Standardize all three subsets of data\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_validation = (X_validation - X_train_mean) / X_train_std\n",
    "X_test = (X_test - X_train_mean) / X_train_std\n",
    "\n",
    "def init_uniform_1_over_sqrt_m(rng, m, n, dtype=np.float32):\n",
    "    # Unif(-1/sqrt(m), 1/sqrt(m))\n",
    "    bound = 1.0 / np.sqrt(m)\n",
    "    W = rng.uniform(-bound, bound, size=(m, n)).astype(dtype)\n",
    "    b = np.zeros((n,), dtype=dtype)  # biases = 0\n",
    "    return W, b\n",
    "\n",
    "def init_uniform_sqrt6_over_m_plus_n(rng, m, n, dtype=np.float32):\n",
    "    # Unif(-sqrt(6/(m+n)), sqrt(6/(m+n)))\n",
    "    bound = np.sqrt(6.0 / (m + n))\n",
    "    W = rng.uniform(-bound, bound, size=(m, n)).astype(dtype)\n",
    "    b = np.zeros((n,), dtype=dtype)  # biases = 0\n",
    "    return W, b\n",
    "\n",
    "def relu(x):  # hidden activation\n",
    "    return np.maximum(x, 0.0)\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - z.max(axis=1, keepdims=True)   # stability\n",
    "    e = np.exp(z)\n",
    "    return e / e.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_onehot(p, t):\n",
    "    return -np.mean(np.sum(t * np.log(p + 1e-12), axis=1))\n",
    "\n",
    "def build_params(layers, init_name, seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    W_list = []\n",
    "    b_list = []\n",
    "    \n",
    "    for i in range(len(layers) - 1):\n",
    "        m = layers[i]     # fan-in\n",
    "        n = layers[i+1]   # fan-out\n",
    "\n",
    "        if init_name == \"uniform_1_sqrt_m\":\n",
    "            W, b = init_uniform_1_over_sqrt_m(rng, m, n)\n",
    "        elif init_name == \"uniform_sqrt6_m_plus_n\":\n",
    "            W, b = init_uniform_sqrt6_over_m_plus_n(rng, m, n)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown init name\")\n",
    "\n",
    "        W_list.append(W)\n",
    "        b_list.append(b)\n",
    "\n",
    "    return {\"W\": W_list, \"b\": b_list}\n",
    "def forward_pass(X, W_list, b_list):\n",
    "    \"\"\"\n",
    "    X      : (N, D)\n",
    "    W_list : [W1, W2, ..., WL]\n",
    "    b_list : [b1, b2, ..., bL]\n",
    "\n",
    "    Returns:\n",
    "        P      : (N, K) softmax probabilities\n",
    "        caches : list of (A_prev, Z) for each layer\n",
    "    \"\"\"\n",
    "    A = X\n",
    "    caches = []\n",
    "\n",
    "    # hidden layers\n",
    "    for i in range(len(W_list) - 1):\n",
    "        Z = A @ W_list[i] + b_list[i]\n",
    "        A_next = relu(Z)\n",
    "        caches.append((A, Z))\n",
    "        A = A_next\n",
    "\n",
    "    # output layer\n",
    "    ZL = A @ W_list[-1] + b_list[-1]\n",
    "    P = softmax(ZL)\n",
    "    caches.append((A, ZL))\n",
    "\n",
    "    return P, caches\n",
    "\n",
    "def backward_pass(P, T, W_list, b_list, caches, lam=0.0):\n",
    "    \"\"\"\n",
    "    P      : (N, K) probs from forward_pass\n",
    "    T      : (N, K) one-hot labels\n",
    "    W_list : [W1, W2, ..., WL]\n",
    "    b_list : [b1, b2, ..., bL]\n",
    "    caches : from forward_pass\n",
    "    lam    : L2 weight decay\n",
    "\n",
    "    Returns:\n",
    "        loss    : scalar CE + L2\n",
    "        grads_W : list of dW for each W\n",
    "        grads_b : list of db for each b\n",
    "    \"\"\"\n",
    "    L = len(W_list)\n",
    "    N = T.shape[0]\n",
    "\n",
    "    # loss = CE + lam * sum ||W||^2\n",
    "    ce = cross_entropy_onehot(P, T)\n",
    "    l2 = sum((W**2).sum() for W in W_list)\n",
    "    loss = ce + lam * l2\n",
    "\n",
    "    grads_W = [None] * L\n",
    "    grads_b = [None] * L\n",
    "\n",
    "    # output layer delta: (P - T)/N\n",
    "    dZ = (P - T) / N\n",
    "    A_prev, ZL = caches[-1]          # last hidden A, output Z\n",
    "    grads_W[-1] = A_prev.T @ dZ + 2*lam*W_list[-1]\n",
    "    grads_b[-1] = dZ.sum(axis=0)\n",
    "\n",
    "    dA = dZ @ W_list[-1].T\n",
    "\n",
    "    # hidden layers: L-2 ... 0\n",
    "    for i in range(L-2, -1, -1):\n",
    "        A_prev, Z = caches[i]\n",
    "        dZ = dA * relu_grad(Z)\n",
    "        grads_W[i] = A_prev.T @ dZ + 2*lam*W_list[i]\n",
    "        grads_b[i] = dZ.sum(axis=0)\n",
    "        dA = dZ @ W_list[i].T\n",
    "\n",
    "    return loss, grads_W, grads_b\n",
    "\n",
    "def evaluate_loss(X, T, W_list, b_list, lam):\n",
    "    \"\"\"\n",
    "    Compute full loss (cross-entropy + L2) on a dataset.\n",
    "    \"\"\"\n",
    "    P, _ = forward_pass(X, W_list, b_list)\n",
    "    ce = cross_entropy_onehot(P, T)\n",
    "    l2 = sum((W**2).sum() for W in W_list)\n",
    "    return ce + lam * l2\n",
    "\n",
    "def predict_classes(X, W_list, b_list):\n",
    "    \"\"\"\n",
    "    Return hard class predictions (0..9) for a dataset.\n",
    "    \"\"\"\n",
    "    P, _ = forward_pass(X, W_list, b_list)\n",
    "    return np.argmax(P, axis=1)\n",
    "\n",
    "def train_one_model(X_train, Y_train,\n",
    "                    X_val,   Y_val,\n",
    "                    init_name, seed_init):\n",
    "    \"\"\"\n",
    "    Train ONE network with a given initialization strategy.\n",
    "    Uses:\n",
    "      - mini-batch gradient descent\n",
    "      - L2 weight decay (lam)\n",
    "      - early stopping based on validation loss\n",
    "    Returns:\n",
    "      W_list, b_list      : trained parameters (best on val)\n",
    "      train_losses        : list of train losses per epoch\n",
    "      val_losses          : list of val losses per epoch\n",
    "    \"\"\"\n",
    "    # initialize parameters\n",
    "    params = build_params(layers, init_name, seed_init)\n",
    "    W_list, b_list = params[\"W\"], params[\"b\"]\n",
    "\n",
    "    rng = np.random.default_rng(seed_init)\n",
    "    N = X_train.shape[0]\n",
    "\n",
    "    best_val = np.inf\n",
    "    best_W, best_b = None, None\n",
    "    wait = 0\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # ---- mini-batch loop ----\n",
    "        idx = np.arange(N)\n",
    "        rng.shuffle(idx)\n",
    "\n",
    "        for start in range(0, N, batch_size):\n",
    "            batch_idx = idx[start:start + batch_size]\n",
    "            xb = X_train[batch_idx]\n",
    "            tb = Y_train[batch_idx]\n",
    "\n",
    "            # forward pass\n",
    "            P, caches = forward_pass(xb, W_list, b_list)\n",
    "\n",
    "            # backward pass (loss + gradients)\n",
    "            loss_batch, grads_W, grads_b = backward_pass(\n",
    "                P, tb, W_list, b_list, caches, lam=lam\n",
    "            )\n",
    "\n",
    "            # gradient descent update\n",
    "            for i in range(len(W_list)):\n",
    "                W_list[i] -= lr * grads_W[i]\n",
    "                b_list[i] -= lr * grads_b[i]\n",
    "\n",
    "        # ---- end of epoch: compute train/val loss ----\n",
    "        train_loss = evaluate_loss(X_train, Y_train, W_list, b_list, lam)\n",
    "        val_loss   = evaluate_loss(X_val,   Y_val,   W_list, b_list, lam)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # ---- early stopping ----\n",
    "        if val_loss + 1e-6 < best_val:\n",
    "            best_val = val_loss\n",
    "            best_W = [W.copy() for W in W_list]\n",
    "            best_b = [b.copy() for b in b_list]\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= early_stopping:\n",
    "                # restore best parameters and stop\n",
    "                W_list = best_W\n",
    "                b_list = best_b\n",
    "                break\n",
    "\n",
    "    return W_list, b_list, train_losses, val_losses\n",
    "\n",
    "def main():\n",
    "    results = {}\n",
    "\n",
    "    # ---------- Model A: Uniform(-1/sqrt(m), 1/sqrt(m)) ----------\n",
    "    W_A, b_A, train_A, val_A = train_one_model(\n",
    "        X_train, Y_train,\n",
    "        X_validation, Y_validation,\n",
    "        init_name=\"uniform_1_sqrt_m\",\n",
    "        seed_init=seed,\n",
    "    )\n",
    "\n",
    "    y_train_pred_A = predict_classes(X_train, W_A, b_A)\n",
    "    y_test_pred_A  = predict_classes(X_test,  W_A, b_A)\n",
    "\n",
    "    train_err_A = 100 * (1 - np.mean(y_train_pred_A == Y_train_orig))\n",
    "    test_err_A  = 100 * (1 - np.mean(y_test_pred_A  == Y_test_orig))\n",
    "\n",
    "    results[\"A\"] = {\n",
    "        \"W\": W_A,\n",
    "        \"b\": b_A,\n",
    "        \"train_losses\": train_A,\n",
    "        \"val_losses\": val_A,\n",
    "        \"train_err\": train_err_A,\n",
    "        \"test_err\": test_err_A,\n",
    "    }\n",
    "\n",
    "    # ---------- Model B: Uniform(-sqrt(6/(m+n)), sqrt(6/(m+n))) ----------\n",
    "    seed_B = seed + 1  # just a different seed\n",
    "    W_B, b_B, train_B, val_B = train_one_model(\n",
    "        X_train, Y_train,\n",
    "        X_validation, Y_validation,\n",
    "        init_name=\"uniform_sqrt6_m_plus_n\",\n",
    "        seed_init=seed_B,\n",
    "    )\n",
    "\n",
    "    y_train_pred_B = predict_classes(X_train, W_B, b_B)\n",
    "    y_test_pred_B  = predict_classes(X_test,  W_B, b_B)\n",
    "\n",
    "    train_err_B = 100 * (1 - np.mean(y_train_pred_B == Y_train_orig))\n",
    "    test_err_B  = 100 * (1 - np.mean(y_test_pred_B  == Y_test_orig))\n",
    "\n",
    "    results[\"B\"] = {\n",
    "        \"W\": W_B,\n",
    "        \"b\": b_B,\n",
    "        \"train_losses\": train_B,\n",
    "        \"val_losses\": val_B,\n",
    "        \"train_err\": train_err_B,\n",
    "        \"test_err\": test_err_B,\n",
    "    }\n",
    "\n",
    "    # ---------- print misclassification errors ----------\n",
    "    print(\"=== Misclassification Errors ===\")\n",
    "    print(\"Model A: Uniform(-1/sqrt(m), 1/sqrt(m))\")\n",
    "    print(f\"  Train error: {train_err_A:.2f}%\")\n",
    "    print(f\"  Test  error: {test_err_A:.2f}%\\n\")\n",
    "\n",
    "    print(\"Model B: Uniform(-sqrt(6/(m+n)), sqrt(6/(m+n)))\")\n",
    "    print(f\"  Train error: {train_err_B:.2f}%\")\n",
    "    print(f\"  Test  error: {test_err_B:.2f}%\\n\")\n",
    "\n",
    "    # ---------- plot loss vs epochs for both models ----------\n",
    "    epochs_A = range(1, len(train_A) + 1)\n",
    "    epochs_B = range(1, len(train_B) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Model A\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epochs_A, train_A, label=\"Train loss (Model A)\")\n",
    "    plt.plot(epochs_A, val_A,   label=\"Validation loss (Model A)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (CE + L2)\")\n",
    "    plt.title(\"Model A: Uniform(-1/sqrt(m), 1/sqrt(m))\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Model B\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs_B, train_B, label=\"Train loss (Model B)\")\n",
    "    plt.plot(epochs_B, val_B,   label=\"Validation loss (Model B)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (CE + L2)\")\n",
    "    plt.title(\"Model B: Uniform(-sqrt(6/(m+n)), sqrt(6/(m+n)))\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb54dc-5cca-4acb-b9c4-889bf7399d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d76d5c-93e9-4527-808f-eb5f5790f328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
